{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36afb123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "spark.conf.set('spark.sql.legacy.timeParserPolicy', 'LEGACY')\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6deaa3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação da sessão spark e definição da variável com a data e hora da execução\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "now = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03e10806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Spark\\spark-3.3.0-bin-hadoop2\\python\\pyspark\\pandas\\utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_parquet`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Leitura do arquivo xlsx em um dataframe pyspark.pandas\n",
    "\n",
    "\n",
    "df_pandas = pd.read_parquet('datalake/bronze/oil')\n",
    "\n",
    "month_dict = {\n",
    "    \"Jan\": 1, \n",
    "    \"Fev\": 2, \n",
    "    \"Mar\": 3, \n",
    "    \"Abr\": 4, \n",
    "    \"Mai\": 5,\n",
    "    \"Jun\": 6, \n",
    "    \"Jul\": 7, \n",
    "    \"Ago\": 8, \n",
    "    \"Set\": 9, \n",
    "    \"Out\": 10, \n",
    "    \"Nov\": 11, \n",
    "    \"Dez\": 12\n",
    "}\n",
    "\n",
    "# Transformação com pd.melt para realizar unpivot parcial do dataframe\n",
    "df_pandas_melt = pd.melt(df_pandas, id_vars = [\"ANO\", \"ESTADO\", \"COMBUSTÍVEL\", \"UNIDADE\"], value_vars = month_dict.keys(), var_name = \"MES\", value_name = \"VOLUME\")\n",
    "df_pandas_melt = df_pandas_melt.fillna(0)\n",
    "df_pandas_melt = df_pandas_melt.replace({\"MES\": month_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a576840f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Spark\\spark-3.3.0-bin-hadoop2\\python\\pyspark\\pandas\\utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Transformação do df pyspark.pandas para spark\n",
    "\n",
    "df_spark = df_pandas_melt.to_spark()\n",
    "\n",
    "# Aplicação da formatação das colunas\n",
    "\n",
    "df_spark = df_spark.withColumn('ANO', f.col('ANO').cast('integer'))\\\n",
    "                   .withColumn('MES', f.when(f.length(f.col('MES')) == 1, f.concat(f.lit('0'), f.col('MES'))).otherwise(f.col('MES')).cast('integer'))\\\n",
    "                   .withColumn('ANO', f.col('ANO').cast('string'))\\\n",
    "                   .withColumn('year_month', f.to_date(f.concat_ws('/', f.col('MES'), f.col('ANO')), 'MM/yyyy'))\\\n",
    "                   .withColumn('uf', f.col('ESTADO').cast('string'))\\\n",
    "                   .withColumn('product', f.regexp_replace('COMBUSTÍVEL', '\\([^\\)]*\\)', '').cast('string'))\\\n",
    "                   .withColumn('unit', f.col('UNIDADE').cast('string'))\\\n",
    "                   .withColumn('volume', f.col('VOLUME').cast('double'))\\\n",
    "                   .withColumn('created_at', f.lit(str(now.strftime(\"%Y-%m-%d %H:%M:%S\"))).cast('timestamp'))\\\n",
    "                   .select('year_month', 'uf', 'product', 'unit', 'volume', 'created_at')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a4e1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year_month: date (nullable = true)\n",
      " |-- uf: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- unit: string (nullable = true)\n",
      " |-- volume: double (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificação do Schema do dataframe\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe1c6cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação do dataframe para validação realizando join entre a base de dados inicial e final\n",
    "\n",
    "df_pandas_validation = df_pandas.to_spark()\n",
    "\n",
    "df_validation = df_spark.groupBy([f.substring(f.to_str('year_month'), 0,4).alias('year'), 'uf', 'product']).sum('volume').withColumnRenamed('sum(volume)', 'totall')\n",
    "\n",
    "df_validation_f = df_validation.join(df_pandas_validation, (df_validation.year == df_pandas_validation.ANO) & (df_validation.uf == df_pandas_validation.ESTADO) & (f.regexp_replace('COMBUSTÍVEL', '\\([^\\)]*\\)', '') == f.col('product')) ,'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24b7dadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+--------------------+------------------+------------------+\n",
      "|year|                 uf|             product|total pré processo|total pós processo|\n",
      "+----+-------------------+--------------------+------------------+------------------+\n",
      "|2020|       MINAS GERAIS|QUEROSENE ILUMINA...|            1250.0|            1400.0|\n",
      "|2020|     RIO DE JANEIRO|QUEROSENE ILUMINA...|            97.996|           106.996|\n",
      "|2020|  RIO GRANDE DO SUL|QUEROSENE ILUMINA...|             464.0|             534.0|\n",
      "|2020|             PARANÁ|QUEROSENE ILUMINA...|              87.0|             112.0|\n",
      "|2020|              BAHIA|QUEROSENE ILUMINA...|             155.4|             160.8|\n",
      "|2020|          SÃO PAULO|QUEROSENE ILUMINA...|             139.8|             164.8|\n",
      "|2020|     ESPÍRITO SANTO|QUEROSENE ILUMINA...|              21.0|              23.0|\n",
      "|2020|RIO GRANDE DO NORTE|QUEROSENE ILUMINA...|              14.0|              18.0|\n",
      "|2020|     SANTA CATARINA|QUEROSENE ILUMINA...|             677.0|             792.0|\n",
      "+----+-------------------+--------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Validação comparando com o dataframe origem do fluxo, procurando as diferenças entre os totais de origem e pós processo\n",
    "df_validation_f.select('year', 'uf', 'product', f.col('TOTAL').alias('total pré processo').cast('float'), f.col('totall').alias('total pós processo').cast('float')).filter(f.col('totall').cast('float') != f.col('TOTAL').cast('float')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b566a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escrita da base em parquet, particionado pela coluna 'product'\n",
    "df_spark.write.mode('overwrite').partitionBy('product').format('parquet').save('datalake/silver/oil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349f3a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
